"""
station_baselines.py
Station-specific THD baselines for anomaly detection.

Each station has different characteristics (local geology, installation, noise).
Instead of universal THD thresholds, we use station-specific baselines and
detect anomalies as z-scores (standard deviations above baseline).

Baselines are established from quiet periods (no known precursors).
Values should be updated periodically as more data is collected.

Author: R.J. Mathews
Date: January 2026
"""

from dataclasses import dataclass
from typing import Dict, Optional
import json
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


@dataclass
class StationBaseline:
    """Baseline statistics for a single station."""
    station: str  # e.g., "IU.ANTO"
    mean_thd: float  # Average THD during quiet period
    std_thd: float  # Standard deviation of THD
    n_samples: int  # Number of days in baseline
    calibration_period: str  # e.g., "2025-06-01 to 2025-12-31"
    notes: str = ""


# Auto-calibrated station baselines (January 2026)
# Generated by: python calibrate_thd_baselines.py --days 30
# Uses robust statistics (median + MAD*1.4826)
STATION_BASELINES: Dict[str, StationBaseline] = {
    # IU Network (40Hz)
    'IU.TUC': StationBaseline(
        station='IU.TUC',
        mean_thd=0.340665,
        std_thd=0.039952,
        n_samples=31,
        calibration_period='2025-12-10 to 2026-01-09',
        notes='Auto-calibrated. QA=acceptable. Serves ridgecrest, socal_saf_mojave, socal_saf_coachella.'
    ),
    'IU.COR': StationBaseline(
        station='IU.COR',
        mean_thd=0.28278,
        std_thd=0.046364,
        n_samples=31,
        calibration_period='2025-12-10 to 2026-01-09',
        notes='Auto-calibrated. QA=acceptable. Cascadia subduction zone.'
    ),
    'IU.MAJO': StationBaseline(
        station='IU.MAJO',
        mean_thd=0.308837,
        std_thd=0.030176,
        n_samples=31,
        calibration_period='2025-12-10 to 2026-01-09',
        notes='Auto-calibrated. QA=acceptable. Tokyo/Kanto fallback (until Hi-net).'
    ),
    'IU.ANTO': StationBaseline(
        station='IU.ANTO',
        mean_thd=0.41292,
        std_thd=0.153212,
        n_samples=30,
        calibration_period='2025-12-10 to 2026-01-09',
        notes='Auto-calibrated. QA=acceptable. Serves istanbul_marmara, turkey_kahramanmaras.'
    ),

    # BK Network (40Hz) - Berkeley
    'BK.BKS': StationBaseline(
        station='BK.BKS',
        mean_thd=0.305512,
        std_thd=0.049728,
        n_samples=31,
        calibration_period='2025-12-10 to 2026-01-09',
        notes='Auto-calibrated. QA=acceptable. NorCal Hayward fault.'
    ),

    # GE Network (20Hz) - GEOFON - Separate baselines due to different sample rate
    'GE.ARPR': StationBaseline(
        station='GE.ARPR',
        mean_thd=1.00,  # UNCALIBRATED - manual estimate
        std_thd=0.30,
        n_samples=7,
        calibration_period='2026-01-01 to 2026-01-07',
        notes='UNCALIBRATED. Arapgir, Turkey. Do not compare to IU stations.'
    ),
    'GE.CSS': StationBaseline(
        station='GE.CSS',
        mean_thd=0.20,  # UNCALIBRATED - manual estimate
        std_thd=0.06,
        n_samples=7,
        calibration_period='2026-01-01 to 2026-01-07',
        notes='UNCALIBRATED. Cyprus. Moderate baseline.'
    ),

    # IV Network - Campi Flegrei (requires INGV access - not available via IRIS)
    # 'IV.CAFE': Not available - requires direct INGV access
}


def get_baseline(station_code: str, network: str) -> Optional[StationBaseline]:
    """
    Get baseline for a station.

    Args:
        station_code: Station code (e.g., 'ANTO')
        network: Network code (e.g., 'IU')

    Returns:
        StationBaseline or None if not found
    """
    key = f"{network}.{station_code}"
    return STATION_BASELINES.get(key)


def compute_z_score(thd_value: float, baseline: StationBaseline) -> float:
    """
    Compute z-score (anomaly score) relative to baseline.

    Returns number of standard deviations above the mean.
    Negative values mean below average (normal).
    Values > 2 are significant anomalies.
    Values > 3 are critical anomalies.
    """
    if baseline.std_thd <= 0:
        return 0.0

    z = (thd_value - baseline.mean_thd) / baseline.std_thd
    return z


def z_score_to_risk(z_score: float) -> float:
    """
    Convert z-score to 0-1 risk score.

    Mapping:
        z < 0: risk = 0.0-0.1 (below baseline)
        z = 0-1: risk = 0.1-0.25 (normal variation)
        z = 1-2: risk = 0.25-0.50 (elevated)
        z = 2-3: risk = 0.50-0.75 (significant anomaly)
        z > 3: risk = 0.75-1.0 (critical anomaly)
    """
    if z_score < 0:
        return max(0.0, 0.1 + z_score * 0.05)  # Clamp to 0
    elif z_score < 1:
        return 0.1 + z_score * 0.15
    elif z_score < 2:
        return 0.25 + (z_score - 1) * 0.25
    elif z_score < 3:
        return 0.50 + (z_score - 2) * 0.25
    else:
        return min(1.0, 0.75 + (z_score - 3) * 0.08)  # Cap at 1.0


def is_baseline_available(station_code: str, network: str) -> bool:
    """Check if baseline is available for station."""
    key = f"{network}.{station_code}"
    return key in STATION_BASELINES


def save_baselines_to_file(filepath: Path):
    """Save current baselines to JSON file."""
    data = {}
    for key, baseline in STATION_BASELINES.items():
        data[key] = {
            'station': baseline.station,
            'mean_thd': baseline.mean_thd,
            'std_thd': baseline.std_thd,
            'n_samples': baseline.n_samples,
            'calibration_period': baseline.calibration_period,
            'notes': baseline.notes
        }

    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

    logger.info(f"Saved {len(data)} station baselines to {filepath}")


def load_baselines_from_file(filepath: Path) -> bool:
    """Load baselines from JSON file, updating the global dict."""
    global STATION_BASELINES

    if not filepath.exists():
        logger.warning(f"Baseline file not found: {filepath}")
        return False

    try:
        with open(filepath, 'r') as f:
            data = json.load(f)

        for key, values in data.items():
            STATION_BASELINES[key] = StationBaseline(
                station=values['station'],
                mean_thd=values['mean_thd'],
                std_thd=values['std_thd'],
                n_samples=values['n_samples'],
                calibration_period=values['calibration_period'],
                notes=values.get('notes', '')
            )

        logger.info(f"Loaded {len(data)} station baselines from {filepath}")
        return True

    except Exception as e:
        logger.error(f"Error loading baselines: {e}")
        return False


# Print summary when imported
if __name__ == "__main__":
    print("Station Baselines Summary")
    print("=" * 70)
    print(f"{'Station':<12} {'Mean THD':>10} {'Std':>8} {'N':>5} {'Notes'}")
    print("-" * 70)
    for key, b in sorted(STATION_BASELINES.items()):
        print(f"{b.station:<12} {b.mean_thd:>10.4f} {b.std_thd:>8.4f} {b.n_samples:>5} {b.notes[:30]}")
